{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/lkDAKgYN1v3pDcaKkvyC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shiladitya56/Machine-Learning/blob/main/SEM3/Natural%20Language%20Processing/lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Shiladitya 2348556 ðŸ¥•"
      ],
      "metadata": {
        "id": "PTqomaRmYvAH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lucCAFPnWulw",
        "outputId": "15eb384c-4b07-4ad5-96df-ab30a8326bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens in Bengali: ['à¦†à¦§à§‡à¦•à¦²à§€à¦¨â€“', 'à¦¹à§ƒà¦¦à¦¯à¦¼à§‡', 'à¦¦à§‚à¦°à¦—à¦¾à¦®à§€\\nà¦¬à§à¦¯à¦¥à¦¾à¦°', 'à¦®à¦¾à¦à§‡', 'à¦˜à§à¦®à¦¿à¦¯à¦¼', 'à¦ªà¦¡à¦¼à¦¿', 'à¦†à¦®à¦¿\\nà¦¸à¦¹à¦¸à¦¾', 'à¦¶à§à¦¨à¦¿', 'à¦°à¦¾à¦¤à§‡à¦°', 'à¦•à¦¡à¦¼à¦¾à¦¨à¦¾à¦¡à¦¼à¦¾\\nà¦…à¦¬à¦¨à§€', ',', 'à¦¬à¦¾à¦¡à¦¼à¦¿', 'à¦†à¦›à§‹', '?']\n",
            "Tokens in German: ['Warum', 'sind', 'alle', 'Deutschen', 'so', 'besessen', 'von', 'Bier', '?', 'Ich', 'liebe', 'Bier', 'aber', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "\n",
        "def tokenize(text, language):\n",
        "    if language == 'bengali':\n",
        "        # Tokenize Bengali text\n",
        "        from indicnlp.tokenize import indic_tokenize\n",
        "        tokens = indic_tokenize.trivial_tokenize(text)\n",
        "    elif language == 'german':\n",
        "        # Tokenize German text\n",
        "        tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+|[^\\w\\s]+')\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported language\")\n",
        "    return tokens\n",
        "\n",
        "bengali_text = \\\n",
        "'''à¦†à¦§à§‡à¦•à¦²à§€à¦¨â€“ à¦¹à§ƒà¦¦à¦¯à¦¼à§‡ à¦¦à§‚à¦°à¦—à¦¾à¦®à§€\n",
        "à¦¬à§à¦¯à¦¥à¦¾à¦° à¦®à¦¾à¦à§‡ à¦˜à§à¦®à¦¿à¦¯à¦¼ à¦ªà¦¡à¦¼à¦¿ à¦†à¦®à¦¿\n",
        "à¦¸à¦¹à¦¸à¦¾ à¦¶à§à¦¨à¦¿ à¦°à¦¾à¦¤à§‡à¦° à¦•à¦¡à¦¼à¦¾à¦¨à¦¾à¦¡à¦¼à¦¾\n",
        "à¦…à¦¬à¦¨à§€, à¦¬à¦¾à¦¡à¦¼à¦¿ à¦†à¦›à§‹?'''\n",
        "\n",
        "german_text = \\\n",
        "\"Warum sind alle Deutschen so besessen von Bier? Ich liebe Bier aber.\"\n",
        "\n",
        "# Tokenize Bengali text\n",
        "bengali_tokens = tokenize(bengali_text, 'bengali')\n",
        "print(\"Tokens in Bengali:\", bengali_tokens)\n",
        "\n",
        "# Tokenize German text\n",
        "german_tokens = tokenize(german_text, 'german')\n",
        "print(\"Tokens in German:\", german_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Tokenization is a fundamental process in natural language processing (NLP) that involves breaking down a piece of text into smaller units, typically words or subwords, called tokens. These tokens serve as the basic building blocks for various NLP tasks such as text analysis, machine translation, sentiment analysis, and information retrieval. Tokenization helps to standardize and structure textual data, enabling computers to understand and process human language more effectively. It involves handling various challenges such as dealing with punctuation marks, handling contractions, and addressing languages with complex word structures. Overall, tokenization plays a crucial role in transforming unstructured text data into a format that can be easily analyzed and utilized by NLP algorithms, facilitating the development of intelligent systems that can comprehend and generate human-like language.\n",
        "\n"
      ],
      "metadata": {
        "id": "yntIoHv1ZP0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization can be done in various ways depending on the requirements of the task and the nature of the text data. Here are some common methods of tokenization:\n",
        "\n",
        "- Word Tokenization: This method breaks text into individual words. It's the most basic form of tokenization and is often used for tasks like language modeling, text classification, and sentiment analysis.\n",
        "\n",
        "- Sentence Tokenization: Sentence tokenization divides text into individual sentences. It's useful for tasks that require analyzing text on a sentence-by-sentence basis, such as machine translation, summarization, and parsing.\n",
        "\n",
        "- Subword Tokenization: Subword tokenization breaks down words into smaller units, such as subword units or characters. It's particularly useful for handling out-of-vocabulary words and languages with complex morphology.\n",
        "\n",
        "- Whitespace Tokenization: This method splits text based on whitespace characters like spaces, tabs, and newlines. It's simple but may not handle all cases correctly, especially in languages with agglutinative or morphologically rich words.\n",
        "\n",
        "- Regular Expression Tokenization: Tokenization using regular expressions allows for more complex patterns to be defined, enabling tokenization based on specific rules or patterns.\n",
        "\n",
        "- Named Entity Recognition (NER) Tokenization: In NER tokenization, text is tokenized in a way that preserves named entities such as person names, organization names, and locations. This is important for tasks like information extraction and named entity recognition.\n",
        "\n",
        "- POS Tagging Tokenization: Tokenization can be combined with part-of-speech (POS) tagging, where each token is tagged with its corresponding part of speech. This is useful for syntactic analysis and semantic understanding.\n",
        "\n",
        "- Dependency Parsing Tokenization: Tokenization can also be integrated with dependency parsing, where tokens are grouped based on their syntactic dependencies. This is useful for understanding the grammatical structure of sentences.\n",
        "\n",
        "- Language-specific Tokenization: Some languages may require specialized tokenization techniques due to their unique characteristics, such as agglutination, word compounding, or lack of clear word boundaries."
      ],
      "metadata": {
        "id": "sJUeVb0LZbba"
      }
    }
  ]
}