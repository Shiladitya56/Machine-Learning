{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shiladitya56/Machine-Learning/blob/main/SEM3/Natural%20Language%20Processing/Similarity_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Shiladitya 2348556 âœŒ"
      ],
      "metadata": {
        "id": "Z5d1hifz08pH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swastikbanerjee/NLP_Lab/blob/main/Similarity_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two paragraphs considered"
      ],
      "metadata": {
        "id": "1mbNaojS8nfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt1=\"Mechanical watches are intricate timepieces that capture the essence of precision craftsmanship and timeless elegance. Unlike their quartz counterparts, which rely on battery-powered electronic movements, mechanical watches operate purely through mechanical means. At the heart of every mechanical watch lies a complex assembly of gears, springs, and levers meticulously crafted to orchestrate the passage of time with unparalleled accuracy. Each component within the movement plays a crucial role, from the mainspring that stores energy to the escapement mechanism that regulates its release, ensuring a smooth and consistent motion of the watch hands.\"\n",
        "txt2=\"Beyond their functional utility, mechanical watches are revered for their artistry and heritage. They serve as tangible symbols of tradition and luxury, embodying centuries-old horological expertise passed down through generations of master watchmakers. Every aspect of a mechanical watch, from its intricate dial design to the painstakingly finished case, reflects a dedication to excellence and attention to detail. Moreover, the rhythmic ticking of a mechanical watch serves as a constant reminder of the craftsmanship and ingenuity that went into its creation, evoking a sense of appreciation for the art of horology. In a world driven by technology and innovation, mechanical watches stand as timeless relics, preserving the rich heritage of traditional watchmaking while continuing to captivate enthusiasts with their beauty and complexity.\""
      ],
      "metadata": {
        "id": "hYNRrFH-7kZJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text1 Preprocessing"
      ],
      "metadata": {
        "id": "EpjZMa_g8rok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# Downloading NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize stemming and lemmatization\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Initialize stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "clean_txt1=[]\n",
        "# Preprocessing steps\n",
        "# Lowercasing\n",
        "txt1 = txt1.lower()\n",
        "# Tokenization\n",
        "tokens = word_tokenize(txt1)\n",
        "# Removing Punctuation, Stopwords, and Numbers\n",
        "tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "# Stemming and Lemmatization\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "# Removing Special Characters\n",
        "cleaned_tokens = [re.sub(r'[^a-zA-Z0-9\\s]', '', token) for token in tokens]\n",
        "# Removing Extra Whitespace\n",
        "cleaned_tokens = [token.strip() for token in cleaned_tokens if token.strip()]\n",
        "# Append preprocessed txt1 to the result\n",
        "clean_txt1.append(cleaned_tokens)\n",
        "# Now, clean_txt contains the preprocessed txt1 data\n",
        "print(clean_txt1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAmJCN_owOXw",
        "outputId": "4ae32e68-ce44-4e7b-97d7-d73755046e9b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['mechanical', 'watches', 'intricate', 'timepieces', 'capture', 'essence', 'precision', 'craftsmanship', 'timeless', 'elegance', 'unlike', 'quartz', 'counterparts', 'rely', 'electronic', 'movements', 'mechanical', 'watches', 'operate', 'purely', 'mechanical', 'means', 'heart', 'every', 'mechanical', 'watch', 'lies', 'complex', 'assembly', 'gears', 'springs', 'levers', 'meticulously', 'crafted', 'orchestrate', 'passage', 'time', 'unparalleled', 'accuracy', 'component', 'within', 'movement', 'plays', 'crucial', 'role', 'mainspring', 'stores', 'energy', 'escapement', 'mechanism', 'regulates', 'release', 'ensuring', 'smooth', 'consistent', 'motion', 'watch', 'hands']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text1 Vectorization"
      ],
      "metadata": {
        "id": "_oyRc-Pw8vuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "# Initialize and train Word2Vec model\n",
        "word2vec_model1 = Word2Vec(sentences=clean_txt1, vector_size=100, window=5, min_count=1, sg=0)\n",
        "# Example of word vector for a specific word\n",
        "word_vector1 = word2vec_model1.wv['timeless']\n",
        "# Print the word vector\n",
        "print(\"Word vector for 'timeless':\", word_vector1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87Dt02FixlD-",
        "outputId": "51c55216-405b-4a57-ddef-829388416286"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word vector for 'timeless': [ 0.00973555 -0.00978038 -0.00649949  0.00278379  0.00643199 -0.00536737\n",
            "  0.00275249  0.00912131 -0.00681542 -0.00609991 -0.00498964 -0.00367641\n",
            "  0.00184972  0.00968263  0.00643778  0.00039709  0.00247077  0.00844049\n",
            "  0.00912898  0.00562875  0.00594626 -0.00762069 -0.00382767 -0.00568033\n",
            "  0.00618177 -0.00225645 -0.00877944  0.00761912  0.00839968 -0.00332024\n",
            "  0.00911666 -0.00073836 -0.00362652 -0.00038469  0.00019443 -0.0035049\n",
            "  0.00281324  0.00572971  0.00686901 -0.00890347 -0.00219273 -0.0054818\n",
            "  0.0075211   0.0065017  -0.00436072  0.00232683 -0.00595366  0.0002365\n",
            "  0.00946176 -0.00260984 -0.00518772 -0.00739721 -0.00291194 -0.00086431\n",
            "  0.00352786  0.00974189 -0.00338928  0.00190177  0.00968101  0.00153159\n",
            "  0.0009865   0.00980237  0.00929546  0.00770807 -0.00617053  0.00998399\n",
            "  0.00584899  0.00907267 -0.0019952   0.00334994  0.00683356 -0.00389376\n",
            "  0.00664285  0.00256286  0.00931373 -0.0030358  -0.00310937  0.00621539\n",
            " -0.00907825 -0.00725399 -0.00650003 -0.00074907 -0.00236302  0.00681552\n",
            "  0.00923659 -0.00090976  0.00141282  0.00202036 -0.0020198  -0.00803434\n",
            "  0.00744105 -0.0042979   0.00457652  0.0090897   0.00304322  0.00313879\n",
            "  0.00406183 -0.00270122  0.00382477  0.00033762]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text2 Preprocessing"
      ],
      "metadata": {
        "id": "0xNwcm8X8ybs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "# why are you running sentences=clean_txt1 for both the paragraphs. i get the value correct.but what is the logic behind that?\n",
        "# Downloading NLTK resources\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "# Initialize stemming and lemmatization\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Initialize stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "clean_txt2=[]\n",
        "# Preprocessing steps\n",
        "# Lowercasing\n",
        "txt2 = txt2.lower()\n",
        "# Tokenization\n",
        "tokens = word_tokenize(txt2)\n",
        "# Removing Punctuation, Stopwords, and Numbers\n",
        "tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "# Stemming and Lemmatization\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "# Removing Special Characters\n",
        "cleaned_tokens = [re.sub(r'[^a-zA-Z0-9\\s]', '', token) for token in tokens]\n",
        "# Removing Extra Whitespace\n",
        "cleaned_tokens = [token.strip() for token in cleaned_tokens if token.strip()]\n",
        "# Append preprocessed txt2 to the result\n",
        "clean_txt2.append(cleaned_tokens)\n",
        "# Now, clean_txt contains the preprocessed txt2 data\n",
        "print(clean_txt2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUIyNKbVxTF8",
        "outputId": "a5931588-dab7-4b10-aeda-cec0d9d1b4f1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['beyond', 'functional', 'utility', 'mechanical', 'watches', 'revered', 'artistry', 'heritage', 'serve', 'tangible', 'symbols', 'tradition', 'luxury', 'embodying', 'horological', 'expertise', 'passed', 'generations', 'master', 'watchmakers', 'every', 'aspect', 'mechanical', 'watch', 'intricate', 'dial', 'design', 'painstakingly', 'finished', 'case', 'reflects', 'dedication', 'excellence', 'attention', 'detail', 'moreover', 'rhythmic', 'ticking', 'mechanical', 'watch', 'serves', 'constant', 'reminder', 'craftsmanship', 'ingenuity', 'went', 'creation', 'evoking', 'sense', 'appreciation', 'art', 'horology', 'world', 'driven', 'technology', 'innovation', 'mechanical', 'watches', 'stand', 'timeless', 'relics', 'preserving', 'rich', 'heritage', 'traditional', 'watchmaking', 'continuing', 'captivate', 'enthusiasts', 'beauty', 'complexity']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text2 Vectorization"
      ],
      "metadata": {
        "id": "gLj6dsBn80wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Initialize and train Word2Vec model\n",
        "word2vec_model2 = Word2Vec(sentences=clean_txt2, vector_size=100, window=5, min_count=1, sg=0)\n",
        "# Another example\n",
        "word_vector2 = word2vec_model2.wv['mechanical']\n",
        "# Print the word vector\n",
        "print(\"Word vector for 'mechanical':\", word_vector2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdnEJowMxczo",
        "outputId": "186d8c0e-33e5-4b69-ecbe-ebe6655f9c8c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word vector for 'mechanical': [-5.2597723e-04  2.4096061e-04  5.0935666e-03  9.0116980e-03\n",
            " -9.3141245e-03 -7.1304860e-03  6.4586308e-03  8.9796288e-03\n",
            " -5.0135073e-03 -3.7730150e-03  7.3810644e-03 -1.5355021e-03\n",
            " -4.5316136e-03  6.5609408e-03 -4.8634983e-03 -1.8216792e-03\n",
            "  2.8879200e-03  9.8803639e-04 -8.2891490e-03 -9.4658714e-03\n",
            "  7.3221549e-03  5.0670197e-03  6.7722313e-03  7.5151084e-04\n",
            "  6.3532470e-03 -3.4100760e-03 -9.6007541e-04  5.7692239e-03\n",
            " -7.5260708e-03 -3.9366111e-03 -7.4952017e-03 -9.3913416e-04\n",
            "  9.5564937e-03 -7.3436527e-03 -2.3405859e-03 -1.9373356e-03\n",
            "  8.0709932e-03 -5.9367651e-03  3.7927515e-05 -4.7680177e-03\n",
            " -9.6055707e-03  5.0092274e-03 -8.7561822e-03 -4.4072666e-03\n",
            " -3.1733747e-05 -2.9355183e-04 -7.6800915e-03  9.6072992e-03\n",
            "  4.9743792e-03  9.2451451e-03 -8.1573287e-03  4.4903010e-03\n",
            " -4.1499482e-03  8.2541793e-04  8.5064424e-03 -4.4686003e-03\n",
            "  4.5259167e-03 -6.7923171e-03 -3.5613847e-03  9.4000464e-03\n",
            " -1.5702010e-03  3.1410012e-04 -4.1387472e-03 -7.6775872e-03\n",
            " -1.5107549e-03  2.4754123e-03 -8.8873849e-04  5.5329935e-03\n",
            " -2.7554147e-03  2.2712909e-03  5.4476196e-03  8.3477851e-03\n",
            " -1.4465145e-03 -9.2056217e-03  4.3849959e-03  5.6153582e-04\n",
            "  7.4389363e-03 -8.1114715e-04 -2.6490183e-03 -8.7509668e-03\n",
            " -8.6196046e-04  2.8330097e-03  5.3907009e-03  7.0691234e-03\n",
            " -5.7170368e-03  1.8519312e-03  6.0945340e-03 -4.8034918e-03\n",
            " -3.1134721e-03  6.7977724e-03  1.6453803e-03  1.8665464e-04\n",
            "  3.4605425e-03  2.1703518e-04  9.6177645e-03  5.0574066e-03\n",
            " -8.9443913e-03 -7.0464164e-03  8.8628294e-04  6.3955453e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combined vectorization, finding avg vector of the two paragraphs and then finding similarity (cosine) between them"
      ],
      "metadata": {
        "id": "znxnFFwr843_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Combine sentences from both clean_txt1 and clean_txt2\n",
        "combined_sentences = clean_txt1 + clean_txt2\n",
        "\n",
        "# Initialize and train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=combined_sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "# Function to calculate average vector representation for a paragraph\n",
        "def get_average_vector(paragraph, model):\n",
        "    vectors = []\n",
        "    for word in paragraph:\n",
        "        if word in model.wv:\n",
        "            vectors.append(model.wv[word])\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Get average vector representations for both paragraphs\n",
        "avg_vector1 = get_average_vector(clean_txt1[0], word2vec_model)\n",
        "avg_vector2 = get_average_vector(clean_txt2[0], word2vec_model)\n",
        "\n",
        "# Calculate cosine similarity between the average vectors\n",
        "if avg_vector1 is not None and avg_vector2 is not None:\n",
        "    similarity_score = cosine_similarity(avg_vector1.reshape(1, -1), avg_vector2.reshape(1, -1))\n",
        "    similarity_percentage = ((similarity_score[0][0] + 1) / 2) * 100\n",
        "    print(\"Similarity between the two paragraphs:\", similarity_percentage, \"%\")\n",
        "else:\n",
        "    print(\"No word vectors found for one or both paragraphs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ69lk1y2bhH",
        "outputId": "e7ea1eec-66cc-449e-980d-26dcb5d8b5b9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between the two paragraphs: 70.82912772893906 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why cosine similarity?"
      ],
      "metadata": {
        "id": "iSm6-CB69NCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic Similarity: Cosine Similarity is effective for measuring the similarity between two vectors in a high-dimensional space, such as the vector representations of the paragraphs obtained from word embeddings like Word2Vec or GloVe. It captures the semantic similarity between the paragraphs based on the direction of the vectors, disregarding their magnitudes.\n",
        "\n",
        "Robustness to Length Differences: Cosine Similarity is robust to differences in the length of the paragraphs. Since it calculates the cosine of the angle between the vectors, it is unaffected by the absolute length of the paragraphs.\n",
        "\n",
        "Contextual Similarity: Cosine Similarity considers the overall context and distribution of words in the paragraphs. It evaluates the similarity based on the distributional semantics of the words, which is suitable for capturing the thematic similarities between the paragraphs.\n",
        "\n",
        "Given the descriptive nature of the paragraphs and their thematic similarity, Cosine Similarity would be a suitable choice for comparing their semantic similarity."
      ],
      "metadata": {
        "id": "DZTP89ru8j9x"
      }
    }
  ]
}