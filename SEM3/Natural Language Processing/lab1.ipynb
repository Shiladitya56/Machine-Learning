{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNUQtfFk77UQIpQ/0TZDVTr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shiladitya56/Machine-Learning/blob/main/SEM3/Natural%20Language%20Processing/lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Shiladitya 2348556 ü¶ã"
      ],
      "metadata": {
        "id": "WR4qkn3ynzcr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfu7QykknERx",
        "outputId": "eb381af0-9491-4fe8-f145-a5e727d90612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization: ['Deep', 'neural', 'networks', 'are', 'highly', 'expressive', 'models', 'that', 'have', 'recently', 'achieved', 'state', 'of', 'the', 'art', 'performance', 'on', 'speech', 'and', 'visual', 'recognition', 'tasks', '.', 'While', 'their', 'expressiveness', 'is', 'the', 'reason', 'they', 'succeed', ',', 'it', 'also', 'causes', 'them', 'to', 'learn', 'uninterpretable', 'solutions', 'that', 'could', 'have', 'counter-intuitive', 'properties', '.', 'First', ',', 'we', 'find', 'that', 'there', 'is', \"n't\", 'any', 'distinction', 'between', 'individual', 'high', 'level', 'units', 'and', 'random', 'linear', 'combinations', 'of', 'high', 'level', 'units', ',', 'according', 'to', 'various', 'methods', 'of', 'unit', 'analysis', '.', 'It', 'suggests', 'that', 'it', 'is', 'the', 'space', ',', 'rather', 'than', 'the', 'individual', 'units', ',', 'that', 'contains', 'the', 'semantic', 'information', 'in', 'the', 'high', 'layers', 'of', 'neural', 'networks', 'üëç', '.', 'Second', ',', 'we', 'find', 'that', 'deep', 'neural', 'networks', 'learn', 'input-output', 'mappings', 'that', 'are', 'fairly', 'discontinuous', 'to', 'a', 'significant', 'extent', '.', 'We', 'can', 'cause', 'the', 'network', 'to', 'misclassify', 'an', 'image', 'by', 'applying', 'a', 'certain', 'hardly', 'perceptible', 'perturbation', 'üôÉ', ',', 'which', 'is', 'found', 'by', 'maximizing', 'the', 'network', '‚Äô', 's', 'prediction', 'error', '.', 'In', 'addition', ',', 'the', 'specific', 'nature', 'of', 'these', 'perturbations', 'is', 'not', 'a', 'random', 'artifact', 'of', 'learning', ':', 'the', 'same', 'perturbation', 'can', 'cause', 'a', 'different', 'network', ',', 'that', 'was', 'trained', 'on', 'a', 'different', 'subset', 'of', 'the', 'dataset', ',', 'to', 'misclassify', 'the', 'same', 'input', '.']\n",
            "Sentence Tokenization: ['Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks.', 'While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties.', \"First, we find that there isn't any distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis.\", 'It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks üëç.', 'Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent.', 'We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation üôÉ, which is found by maximizing the network‚Äôs prediction error.', 'In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.']\n",
            "Punctuation-based Tokenizer: ['Deep', 'neural', 'networks', 'are', 'highly', 'expressive', 'models', 'that', 'have', 'recently', 'achieved', 'state', 'of', 'the', 'art', 'performance', 'on', 'speech', 'and', 'visual', 'recognition', 'tasks', '.', 'While', 'their', 'expressiveness', 'is', 'the', 'reason', 'they', 'succeed', ',', 'it', 'also', 'causes', 'them', 'to', 'learn', 'uninterpretable', 'solutions', 'that', 'could', 'have', 'counter', '-', 'intuitive', 'properties', '.', 'First', ',', 'we', 'find', 'that', 'there', 'isn', \"'\", 't', 'any', 'distinction', 'between', 'individual', 'high', 'level', 'units', 'and', 'random', 'linear', 'combinations', 'of', 'high', 'level', 'units', ',', 'according', 'to', 'various', 'methods', 'of', 'unit', 'analysis', '.', 'It', 'suggests', 'that', 'it', 'is', 'the', 'space', ',', 'rather', 'than', 'the', 'individual', 'units', ',', 'that', 'contains', 'the', 'semantic', 'information', 'in', 'the', 'high', 'layers', 'of', 'neural', 'networks', 'üëç.', 'Second', ',', 'we', 'find', 'that', 'deep', 'neural', 'networks', 'learn', 'input', '-', 'output', 'mappings', 'that', 'are', 'fairly', 'discontinuous', 'to', 'a', 'significant', 'extent', '.', 'We', 'can', 'cause', 'the', 'network', 'to', 'misclassify', 'an', 'image', 'by', 'applying', 'a', 'certain', 'hardly', 'perceptible', 'perturbation', 'üôÉ,', 'which', 'is', 'found', 'by', 'maximizing', 'the', 'network', '‚Äô', 's', 'prediction', 'error', '.', 'In', 'addition', ',', 'the', 'specific', 'nature', 'of', 'these', 'perturbations', 'is', 'not', 'a', 'random', 'artifact', 'of', 'learning', ':', 'the', 'same', 'perturbation', 'can', 'cause', 'a', 'different', 'network', ',', 'that', 'was', 'trained', 'on', 'a', 'different', 'subset', 'of', 'the', 'dataset', ',', 'to', 'misclassify', 'the', 'same', 'input', '.']\n",
            "Treebank Word tokenizer: ['Deep', 'neural', 'networks', 'are', 'highly', 'expressive', 'models', 'that', 'have', 'recently', 'achieved', 'state', 'of', 'the', 'art', 'performance', 'on', 'speech', 'and', 'visual', 'recognition', 'tasks.', 'While', 'their', 'expressiveness', 'is', 'the', 'reason', 'they', 'succeed', ',', 'it', 'also', 'causes', 'them', 'to', 'learn', 'uninterpretable', 'solutions', 'that', 'could', 'have', 'counter-intuitive', 'properties.', 'First', ',', 'we', 'find', 'that', 'there', 'is', \"n't\", 'any', 'distinction', 'between', 'individual', 'high', 'level', 'units', 'and', 'random', 'linear', 'combinations', 'of', 'high', 'level', 'units', ',', 'according', 'to', 'various', 'methods', 'of', 'unit', 'analysis.', 'It', 'suggests', 'that', 'it', 'is', 'the', 'space', ',', 'rather', 'than', 'the', 'individual', 'units', ',', 'that', 'contains', 'the', 'semantic', 'information', 'in', 'the', 'high', 'layers', 'of', 'neural', 'networks', 'üëç.', 'Second', ',', 'we', 'find', 'that', 'deep', 'neural', 'networks', 'learn', 'input-output', 'mappings', 'that', 'are', 'fairly', 'discontinuous', 'to', 'a', 'significant', 'extent.', 'We', 'can', 'cause', 'the', 'network', 'to', 'misclassify', 'an', 'image', 'by', 'applying', 'a', 'certain', 'hardly', 'perceptible', 'perturbation', 'üôÉ', ',', 'which', 'is', 'found', 'by', 'maximizing', 'the', 'network‚Äôs', 'prediction', 'error.', 'In', 'addition', ',', 'the', 'specific', 'nature', 'of', 'these', 'perturbations', 'is', 'not', 'a', 'random', 'artifact', 'of', 'learning', ':', 'the', 'same', 'perturbation', 'can', 'cause', 'a', 'different', 'network', ',', 'that', 'was', 'trained', 'on', 'a', 'different', 'subset', 'of', 'the', 'dataset', ',', 'to', 'misclassify', 'the', 'same', 'input', '.']\n",
            "Tweet Tokenizer: ['Deep', 'neural', 'networks', 'are', 'highly', 'expressive', 'models', 'that', 'have', 'recently', 'achieved', 'state', 'of', 'the', 'art', 'performance', 'on', 'speech', 'and', 'visual', 'recognition', 'tasks', '.', 'While', 'their', 'expressiveness', 'is', 'the', 'reason', 'they', 'succeed', ',', 'it', 'also', 'causes', 'them', 'to', 'learn', 'uninterpretable', 'solutions', 'that', 'could', 'have', 'counter-intuitive', 'properties', '.', 'First', ',', 'we', 'find', 'that', 'there', \"isn't\", 'any', 'distinction', 'between', 'individual', 'high', 'level', 'units', 'and', 'random', 'linear', 'combinations', 'of', 'high', 'level', 'units', ',', 'according', 'to', 'various', 'methods', 'of', 'unit', 'analysis', '.', 'It', 'suggests', 'that', 'it', 'is', 'the', 'space', ',', 'rather', 'than', 'the', 'individual', 'units', ',', 'that', 'contains', 'the', 'semantic', 'information', 'in', 'the', 'high', 'layers', 'of', 'neural', 'networks', 'üëç', '.', 'Second', ',', 'we', 'find', 'that', 'deep', 'neural', 'networks', 'learn', 'input-output', 'mappings', 'that', 'are', 'fairly', 'discontinuous', 'to', 'a', 'significant', 'extent', '.', 'We', 'can', 'cause', 'the', 'network', 'to', 'misclassify', 'an', 'image', 'by', 'applying', 'a', 'certain', 'hardly', 'perceptible', 'perturbation', 'üôÉ', ',', 'which', 'is', 'found', 'by', 'maximizing', 'the', 'network', '‚Äô', 's', 'prediction', 'error', '.', 'In', 'addition', ',', 'the', 'specific', 'nature', 'of', 'these', 'perturbations', 'is', 'not', 'a', 'random', 'artifact', 'of', 'learning', ':', 'the', 'same', 'perturbation', 'can', 'cause', 'a', 'different', 'network', ',', 'that', 'was', 'trained', 'on', 'a', 'different', 'subset', 'of', 'the', 'dataset', ',', 'to', 'misclassify', 'the', 'same', 'input', '.']\n",
            "Multi-Word Expression Tokenizer: ['Deep', 'neural', 'networks', 'are', 'highly', 'expressive', 'models', 'that', 'have', 'recently', 'achieved', 'state', 'of', 'the', 'art', 'performance', 'on', 'speech', 'and', 'visual', 'recognition', 'tasks', '.', 'While', 'their', 'expressiveness', 'is', 'the', 'reason', 'they', 'succeed', ',', 'it', 'also', 'causes', 'them', 'to', 'learn', 'uninterpretable', 'solutions', 'that', 'could', 'have', 'counter-intuitive', 'properties', '.', 'First', ',', 'we', 'find', 'that', 'there', 'is', \"n't\", 'any', 'distinction', 'between', 'individual', 'high', 'level', 'units', 'and', 'random', 'linear', 'combinations', 'of', 'high', 'level', 'units', ',', 'according', 'to', 'various', 'methods', 'of', 'unit', 'analysis', '.', 'It', 'suggests', 'that', 'it', 'is', 'the', 'space', ',', 'rather', 'than', 'the', 'individual', 'units', ',', 'that', 'contains', 'the', 'semantic', 'information', 'in', 'the', 'high', 'layers', 'of', 'neural', 'networks', 'üëç', '.', 'Second', ',', 'we', 'find', 'that', 'deep', 'neural', 'networks', 'learn', 'input-output', 'mappings', 'that', 'are', 'fairly', 'discontinuous', 'to', 'a', 'significant', 'extent', '.', 'We', 'can', 'cause', 'the', 'network', 'to', 'misclassify', 'an', 'image', 'by', 'applying', 'a', 'certain', 'hardly', 'perceptible', 'perturbation', 'üôÉ', ',', 'which', 'is', 'found', 'by', 'maximizing', 'the', 'network', '‚Äô', 's', 'prediction', 'error', '.', 'In', 'addition', ',', 'the', 'specific', 'nature', 'of', 'these', 'perturbations', 'is', 'not', 'a', 'random', 'artifact', 'of', 'learning', ':', 'the', 'same', 'perturbation', 'can', 'cause', 'a', 'different', 'network', ',', 'that', 'was', 'trained', 'on', 'a', 'different', 'subset', 'of', 'the', 'dataset', ',', 'to', 'misclassify', 'the', 'same', 'input', '.']\n",
            "TextBlob Word Tokenize: ['Deep', 'neural', 'networks', 'are', 'highly', 'expressive', 'models', 'that', 'have', 'recently', 'achieved', 'state', 'of', 'the', 'art', 'performance', 'on', 'speech', 'and', 'visual', 'recognition', 'tasks', 'While', 'their', 'expressiveness', 'is', 'the', 'reason', 'they', 'succeed', 'it', 'also', 'causes', 'them', 'to', 'learn', 'uninterpretable', 'solutions', 'that', 'could', 'have', 'counter-intuitive', 'properties', 'First', 'we', 'find', 'that', 'there', 'is', \"n't\", 'any', 'distinction', 'between', 'individual', 'high', 'level', 'units', 'and', 'random', 'linear', 'combinations', 'of', 'high', 'level', 'units', 'according', 'to', 'various', 'methods', 'of', 'unit', 'analysis', 'It', 'suggests', 'that', 'it', 'is', 'the', 'space', 'rather', 'than', 'the', 'individual', 'units', 'that', 'contains', 'the', 'semantic', 'information', 'in', 'the', 'high', 'layers', 'of', 'neural', 'networks', 'üëç', 'Second', 'we', 'find', 'that', 'deep', 'neural', 'networks', 'learn', 'input-output', 'mappings', 'that', 'are', 'fairly', 'discontinuous', 'to', 'a', 'significant', 'extent', 'We', 'can', 'cause', 'the', 'network', 'to', 'misclassify', 'an', 'image', 'by', 'applying', 'a', 'certain', 'hardly', 'perceptible', 'perturbation', 'üôÉ', 'which', 'is', 'found', 'by', 'maximizing', 'the', 'network', '‚Äô', 's', 'prediction', 'error', 'In', 'addition', 'the', 'specific', 'nature', 'of', 'these', 'perturbations', 'is', 'not', 'a', 'random', 'artifact', 'of', 'learning', 'the', 'same', 'perturbation', 'can', 'cause', 'a', 'different', 'network', 'that', 'was', 'trained', 'on', 'a', 'different', 'subset', 'of', 'the', 'dataset', 'to', 'misclassify', 'the', 'same', 'input']\n",
            "spaCy Tokenizer: ['Deep', 'neural', 'networks', 'are', 'highly', 'expressive', 'models', 'that', 'have', 'recently', 'achieved', 'state', 'of', 'the', 'art', 'performance', 'on', 'speech', 'and', 'visual', 'recognition', 'tasks', '.', 'While', 'their', 'expressiveness', 'is', 'the', 'reason', 'they', 'succeed', ',', 'it', 'also', 'causes', 'them', 'to', 'learn', 'uninterpretable', 'solutions', 'that', 'could', 'have', 'counter', '-', 'intuitive', 'properties', '.', 'First', ',', 'we', 'find', 'that', 'there', 'is', \"n't\", 'any', 'distinction', 'between', 'individual', 'high', 'level', 'units', 'and', 'random', 'linear', 'combinations', 'of', 'high', 'level', 'units', ',', 'according', 'to', 'various', 'methods', 'of', 'unit', 'analysis', '.', 'It', 'suggests', 'that', 'it', 'is', 'the', 'space', ',', 'rather', 'than', 'the', 'individual', 'units', ',', 'that', 'contains', 'the', 'semantic', 'information', 'in', 'the', 'high', 'layers', 'of', 'neural', 'networks', 'üëç', '.', 'Second', ',', 'we', 'find', 'that', 'deep', 'neural', 'networks', 'learn', 'input', '-', 'output', 'mappings', 'that', 'are', 'fairly', 'discontinuous', 'to', 'a', 'significant', 'extent', '.', 'We', 'can', 'cause', 'the', 'network', 'to', 'misclassify', 'an', 'image', 'by', 'applying', 'a', 'certain', 'hardly', 'perceptible', 'perturbation', 'üôÉ', ',', 'which', 'is', 'found', 'by', 'maximizing', 'the', 'network', '‚Äôs', 'prediction', 'error', '.', 'In', 'addition', ',', 'the', 'specific', 'nature', 'of', 'these', 'perturbations', 'is', 'not', 'a', 'random', 'artifact', 'of', 'learning', ':', 'the', 'same', 'perturbation', 'can', 'cause', 'a', 'different', 'network', ',', 'that', 'was', 'trained', 'on', 'a', 'different', 'subset', 'of', 'the', 'dataset', ',', 'to', 'misclassify', 'the', 'same', 'input', '.']\n",
            "Gensim word tokenizer: ['Deep', 'neural', 'networks', 'are', 'highly', 'expressive', 'models', 'that', 'have', 'recently', 'achieved', 'state', 'of', 'the', 'art', 'performance', 'on', 'speech', 'and', 'visual', 'recognition', 'tasks', 'While', 'their', 'expressiveness', 'is', 'the', 'reason', 'they', 'succeed', 'it', 'also', 'causes', 'them', 'to', 'learn', 'uninterpretable', 'solutions', 'that', 'could', 'have', 'counter', 'intuitive', 'properties', 'First', 'we', 'find', 'that', 'there', 'isn', 't', 'any', 'distinction', 'between', 'individual', 'high', 'level', 'units', 'and', 'random', 'linear', 'combinations', 'of', 'high', 'level', 'units', 'according', 'to', 'various', 'methods', 'of', 'unit', 'analysis', 'It', 'suggests', 'that', 'it', 'is', 'the', 'space', 'rather', 'than', 'the', 'individual', 'units', 'that', 'contains', 'the', 'semantic', 'information', 'in', 'the', 'high', 'layers', 'of', 'neural', 'networks', 'Second', 'we', 'find', 'that', 'deep', 'neural', 'networks', 'learn', 'input', 'output', 'mappings', 'that', 'are', 'fairly', 'discontinuous', 'to', 'a', 'significant', 'extent', 'We', 'can', 'cause', 'the', 'network', 'to', 'misclassify', 'an', 'image', 'by', 'applying', 'a', 'certain', 'hardly', 'perceptible', 'perturbation', 'which', 'is', 'found', 'by', 'maximizing', 'the', 'network', 's', 'prediction', 'error', 'In', 'addition', 'the', 'specific', 'nature', 'of', 'these', 'perturbations', 'is', 'not', 'a', 'random', 'artifact', 'of', 'learning', 'the', 'same', 'perturbation', 'can', 'cause', 'a', 'different', 'network', 'that', 'was', 'trained', 'on', 'a', 'different', 'subset', 'of', 'the', 'dataset', 'to', 'misclassify', 'the', 'same', 'input']\n",
            "Tokenization with Keras: ['deep', 'neural', 'networks', 'are', 'highly', 'expressive', 'models', 'that', 'have', 'recently', 'achieved', 'state', 'of', 'the', 'art', 'performance', 'on', 'speech', 'and', 'visual', 'recognition', 'tasks', 'while', 'their', 'expressiveness', 'is', 'the', 'reason', 'they', 'succeed', 'it', 'also', 'causes', 'them', 'to', 'learn', 'uninterpretable', 'solutions', 'that', 'could', 'have', 'counter', 'intuitive', 'properties', 'first', 'we', 'find', 'that', 'there', \"isn't\", 'any', 'distinction', 'between', 'individual', 'high', 'level', 'units', 'and', 'random', 'linear', 'combinations', 'of', 'high', 'level', 'units', 'according', 'to', 'various', 'methods', 'of', 'unit', 'analysis', 'it', 'suggests', 'that', 'it', 'is', 'the', 'space', 'rather', 'than', 'the', 'individual', 'units', 'that', 'contains', 'the', 'semantic', 'information', 'in', 'the', 'high', 'layers', 'of', 'neural', 'networks', 'üëç', 'second', 'we', 'find', 'that', 'deep', 'neural', 'networks', 'learn', 'input', 'output', 'mappings', 'that', 'are', 'fairly', 'discontinuous', 'to', 'a', 'significant', 'extent', 'we', 'can', 'cause', 'the', 'network', 'to', 'misclassify', 'an', 'image', 'by', 'applying', 'a', 'certain', 'hardly', 'perceptible', 'perturbation', 'üôÉ', 'which', 'is', 'found', 'by', 'maximizing', 'the', 'network‚Äôs', 'prediction', 'error', 'in', 'addition', 'the', 'specific', 'nature', 'of', 'these', 'perturbations', 'is', 'not', 'a', 'random', 'artifact', 'of', 'learning', 'the', 'same', 'perturbation', 'can', 'cause', 'a', 'different', 'network', 'that', 'was', 'trained', 'on', 'a', 'different', 'subset', 'of', 'the', 'dataset', 'to', 'misclassify', 'the', 'same', 'input']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer, MWETokenizer\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "from gensim.utils import tokenize\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "text = \"Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. First, we find that there isn't any distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks üëç. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation üôÉ, which is found by maximizing the network‚Äôs prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.\"\n",
        "\n",
        "# a. Word Tokenization\n",
        "nltk.download('punkt')\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word Tokenization:\", word_tokens)\n",
        "\n",
        "# b. Sentence Tokenization\n",
        "sent_tokens = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\", sent_tokens)\n",
        "\n",
        "# c. Punctuation-based Tokenizer\n",
        "punctuation_tokens = nltk.tokenize.WordPunctTokenizer().tokenize(text)\n",
        "print(\"Punctuation-based Tokenizer:\", punctuation_tokens)\n",
        "\n",
        "# d. Treebank Word tokenizer\n",
        "treebank_tokenizer = TreebankWordTokenizer().tokenize(text)\n",
        "print(\"Treebank Word tokenizer:\", treebank_tokenizer)\n",
        "\n",
        "# e. Tweet Tokenizer\n",
        "tweet_tokenizer = TweetTokenizer().tokenize(text)\n",
        "print(\"Tweet Tokenizer:\", tweet_tokenizer)\n",
        "\n",
        "# f. Multi-Word Expression Tokenizer\n",
        "mwe_tokenizer = MWETokenizer([('Word', 'Tokenization'), ('Tokenization', 'Python')])\n",
        "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(text))\n",
        "print(\"Multi-Word Expression Tokenizer:\", mwe_tokens)\n",
        "\n",
        "# g. TextBlob Word Tokenize\n",
        "textblob_tokens = TextBlob(text).words\n",
        "print(\"TextBlob Word Tokenize:\", textblob_tokens)\n",
        "\n",
        "# h. spaCy Tokenizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "print(\"spaCy Tokenizer:\", spacy_tokens)\n",
        "\n",
        "# i. Gensim word tokenizer\n",
        "gensim_tokens = list(tokenize(text))\n",
        "print(\"Gensim word tokenizer:\", gensim_tokens)\n",
        "\n",
        "# j. Tokenization with Keras\n",
        "keras_tokens = text_to_word_sequence(text)\n",
        "print(\"Tokenization with Keras:\", keras_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "a. **Word Tokenization**:\n",
        "   - Definition: Word tokenization is the process of splitting a text into individual words or tokens based on whitespace or punctuation.\n",
        "   - Example:\n",
        "     Input: \"Hello, world! How are you?\"\n",
        "     Output: ['Hello', ',', 'world', '!', 'How', 'are', 'you', '?']\n",
        "\n",
        "b. **Sentence Tokenization**:\n",
        "   - Definition: Sentence tokenization is the process of splitting a text into individual sentences.\n",
        "   - Example:\n",
        "     Input: \"Hello, world! How are you? I'm doing great.\"\n",
        "     Output: [\"Hello, world!\", \"How are you?\", \"I'm doing great.\"]\n",
        "\n",
        "c. **Punctuation-based Tokenizer**:\n",
        "   - Definition: Punctuation-based tokenizer splits a text into tokens based on punctuation marks such as commas, periods, etc.\n",
        "   - Example:\n",
        "     Input: \"Hello, world! How are you?\"\n",
        "     Output: ['Hello', ',', 'world', '!', 'How', 'are', 'you', '?']\n",
        "\n",
        "d. **Treebank Word Tokenizer**:\n",
        "   - Definition: Treebank word tokenizer is a word tokenizer that follows the conventions used in the Penn Treebank corpus.\n",
        "   - Example:\n",
        "     Input: \"Hello, world! How are you?\"\n",
        "     Output: ['Hello', ',', 'world', '!', 'How', 'are', 'you', '?']\n",
        "\n",
        "e. **Tweet Tokenizer**:\n",
        "   - Definition: Tweet tokenizer is specialized for tokenizing tweets, considering emojis, hashtags, and handles.\n",
        "   - Example:\n",
        "     Input: \"Just landed ‚úàÔ∏è in New York! #excited\"\n",
        "     Output: ['Just', 'landed', '‚úàÔ∏è', 'in', 'New', 'York', '!', '#excited']\n",
        "\n",
        "f. **Multi-Word Expression Tokenizer**:\n",
        "   - Definition: Multi-Word Expression tokenizer recognizes specific multi-word expressions and treats them as single tokens.\n",
        "   - Example:\n",
        "     Input: \"New York City is a great place to visit.\"\n",
        "     Output: ['New York City', 'is', 'a', 'great', 'place', 'to', 'visit']\n",
        "\n",
        "g. **TextBlob Word Tokenize**:\n",
        "   - Definition: TextBlob word tokenize is used to tokenize text into individual words using the TextBlob library.\n",
        "   - Example:\n",
        "     Input: \"Hello, world! How are you?\"\n",
        "     Output: ['Hello', 'world', 'How', 'are', 'you']\n",
        "\n",
        "h. **spaCy Tokenizer**:\n",
        "   - Definition: spaCy tokenizer tokenizes the text using the spaCy library, providing detailed tokenization including parts of speech tagging and entity recognition.\n",
        "   - Example:\n",
        "     Input: \"Hello, world! How are you?\"\n",
        "     Output: ['Hello', ',', 'world', '!', 'How', 'are', 'you', '?']\n",
        "\n",
        "i. **Gensim word tokenizer**:\n",
        "   - Definition: Gensim word tokenizer is a simple tokenizer provided by the Gensim library.\n",
        "   - Example:\n",
        "     Input: \"Hello, world! How are you?\"\n",
        "     Output: ['Hello', 'world', 'How', 'are', 'you']\n",
        "\n",
        "j. **Tokenization with Keras**:\n",
        "   - Definition: Tokenization with Keras is similar to word tokenization but converts the text to lowercase and removes punctuation.\n",
        "   - Example:\n",
        "     Input: \"Hello, world! How are you?\"\n",
        "     Output: ['hello', 'world', 'how', 'are', 'you']"
      ],
      "metadata": {
        "id": "y-K3PrJ3nmEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Word Tokenization**:\n",
        "   - Insight: Word tokenization is a fundamental step in natural language processing (NLP) tasks, breaking down text into its constituent words.\n",
        "   - Applications: Sentiment analysis, text classification, language translation, named entity recognition.\n",
        "\n",
        "2. **Sentence Tokenization**:\n",
        "   - Insight: Sentence tokenization divides text into individual sentences, enabling analysis at a higher level of granularity.\n",
        "   - Applications: Text summarization, machine translation, text-to-speech systems, document indexing.\n",
        "\n",
        "3. **Punctuation-based Tokenizer**:\n",
        "   - Insight: Punctuation-based tokenizer segments text based on punctuation marks, preserving them as separate tokens.\n",
        "   - Applications: Sentiment analysis, parsing URLs or email addresses, analyzing text with emoticons or special characters.\n",
        "\n",
        "4. **Treebank Word Tokenizer**:\n",
        "   - Insight: Treebank word tokenizer follows conventions used in the Penn Treebank corpus, providing standardized tokenization.\n",
        "   - Applications: Part-of-speech tagging, dependency parsing, named entity recognition, syntactic analysis.\n",
        "\n",
        "5. **Tweet Tokenizer**:\n",
        "   - Insight: Tweet tokenizer handles tokenization in the context of social media text, which often contains hashtags, mentions, and emojis.\n",
        "   - Applications: Social media sentiment analysis, trend analysis, user profiling, brand monitoring.\n",
        "\n",
        "6. **Multi-Word Expression Tokenizer**:\n",
        "   - Insight: Multi-word expression tokenizer recognizes and treats multi-word phrases as single tokens, preserving their semantic integrity.\n",
        "   - Applications: Named entity recognition, extracting domain-specific terminology, phrase-based machine translation.\n",
        "\n",
        "7. **TextBlob Word Tokenize**:\n",
        "   - Insight: TextBlob word tokenize provides a simple and easy-to-use tokenization method integrated with other NLP functionalities.\n",
        "   - Applications: Basic NLP tasks like sentiment analysis, part-of-speech tagging, text classification.\n",
        "\n",
        "8. **spaCy Tokenizer**:\n",
        "   - Insight: spaCy tokenizer offers detailed tokenization with linguistic features such as part-of-speech tagging and named entity recognition.\n",
        "   - Applications: Advanced NLP tasks including entity recognition, information extraction, syntactic parsing.\n",
        "\n",
        "9. **Gensim word tokenizer**:\n",
        "   - Insight: Gensim word tokenizer is a lightweight tool for basic tokenization tasks, particularly suited for large-scale text processing.\n",
        "   - Applications: Topic modeling, document clustering, word embedding generation, semantic similarity calculation.\n",
        "\n",
        "10. **Tokenization with Keras**:\n",
        "    - Insight: Tokenization with Keras is tailored for text preprocessing in deep learning models, converting text to lowercase and removing punctuation.\n",
        "    - Applications: Text classification, sentiment analysis, sequence-to-sequence tasks, neural machine translation."
      ],
      "metadata": {
        "id": "HEX27l6dnpWL"
      }
    }
  ]
}